{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612a69a7-25f1-48f5-b8aa-7385082dad73",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Linear Regression - Theory\n",
    "Ref: ```\"Statistical Methods for the Earth Scientist: An Introduction\" R. Till, 1974```\n",
    "  \n",
    "This will be a brief, non-statistically thorough, introduction to the terminology and mathematical syntax of linear regression.   \n",
    "\n",
    "## Scalar, Vector and Matrix Nomenclature being used\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "Variables - lower case:     ${x}$  \n",
    "</div>\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "A Vector - lower case bold: $\\mathbf{x}$  \n",
    "    \n",
    "$$\n",
    "\\mathbf{x} = \\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\\mathbf{x}^T = \\begin{pmatrix}\n",
    "x_1 & x_2 & \\cdots & x_n\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "</div>\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "Matrices - upper case bold: $\\mathbf{X}$  of size ${m}$ rows by ${n}$ columns\n",
    "    \n",
    "$$\\mathbf{ X_{m \\times n}} = \\begin{pmatrix}\n",
    "\\mathbf{x_1}^T \\\\\n",
    "\\mathbf{x_2}^T \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{x_m}^T\n",
    "\\end{pmatrix}\n",
    "$$   \n",
    "</div>\n",
    "\n",
    "## What is Linear Regression?\n",
    "\n",
    "Linear regression is one of the most fundamental and widely used supervised machine learning algorithms. It models the relationship between a dependent variable (target) and one or more *independent* variables (features) by fitting a linear equation to the observed data.\n",
    "\n",
    "#### The basic form of a linear regression equation is:\n",
    "\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "{y} = {\\beta}_{0} + {x}_{1} {\\beta}_{1} + {x}_{2} {\\beta}_{2}  + ... + {x}_{n} {\\beta}_{n}  + {\\epsilon}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "Where:\n",
    "- ${y}$ is the dependent variable that we're trying to predict\n",
    "- ${x}_{1}$, ${x}_{2}$, ..., ${x}_{n}$ are the independent variables\n",
    "- ${\\beta}_{0}$ is the y-intercept (bias)\n",
    "- ${\\beta}_{1}$, ${\\beta}_{2}$, ..., ${\\beta}_{n}$ are the coefficients (weights) for each feature\n",
    "- ${\\epsilon}$ is the error term\n",
    "\n",
    "#### In matrix form:\n",
    "\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "</div>\n",
    "    \n",
    "$\\mathbf{y}$ &emsp;&emsp;A (column) vector of responses<br>\n",
    "$\\mathbf{X}$&emsp;&emsp; A matrix of independent variables (one row for each sample)<br>\n",
    "$\\boldsymbol{\\beta}$&emsp;&emsp; A vector of coefficients.<br>\n",
    "\n",
    "#### Equation to solve for ${\\beta}$:\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "\\boldsymbol{\\beta} = (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "## Importance in Machine Learning\n",
    "\n",
    "Linear regression is important for several reasons:\n",
    "\n",
    "1. **Simplicity**: It's easy to understand and implement\n",
    "2. **Interpretability**: The coefficients directly tell us how much each feature affects the target\n",
    "3. **Foundation**: It serves as a building block for more complex algorithms\n",
    "4. **Efficiency**: It requires minimal computational resources\n",
    "5. **Baseline**: Often used as a benchmark against which to compare more complex models\n",
    "\n",
    "## Types of Linear Regression\n",
    "\n",
    "### 1. Simple Linear Regression\n",
    "\n",
    "Simple linear regression involves only one independent variable to predict the target variable.\n",
    "\n",
    "**Equation**: <div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "{y} = {\\beta}_{0} + {x}_{1} {\\beta}_{1}  + {\\epsilon}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "**Use Cases**:\n",
    "- Predicting sales based on advertising budget\n",
    "- Forecasting house prices based on square footage\n",
    "- Estimating crop yield based on rainfall\n",
    "\n",
    "NOTE: Python example in a separate file. \n",
    "<!-- [Regression Python worksheet](\"./Linear Regression - Simple- with matrix equation.ipynb\") -->\n",
    "\n",
    "### 2. Multiple Linear Regression\n",
    "\n",
    "Multiple linear regression involves two or more independent variables to predict the target variable\n",
    "\n",
    "**Equation**: <div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "{y} = {\\beta}_{0} + {x}_{1} {\\beta}_{1} + {x}_{2} {\\beta}_{2}  + ... + {x}_{n} {\\beta}_{n}  + {\\epsilon}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "**Use Cases**:\n",
    "- Predicting house prices based multiple predictors, such as size, location, age, etc.\n",
    "- Forecasting company revenue based on various economic indicators\n",
    "- Determining product pricing based on multiple factors\n",
    "\n",
    "NOTE: Python example in a separate file.\n",
    "\n",
    "### 3. Polynomial Regression\n",
    "\n",
    "While not strictly a different type of linear regression, polynomial regression extends linear regression by adding polynomial terms.\n",
    "\n",
    "**Equation**: \n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "{y} = {\\beta}_{0} + {x}_{1} {\\beta}_{1} + {x}_{1}^2 {\\beta}_{2}  + ... + {x}_{1}^n  {\\beta}_{n} + {\\epsilon}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "**Use Cases**:\n",
    "- Modeling data with non-linear relationships\n",
    "- Capturing diminishing returns (e.g., advertising spend)\n",
    "- Modeling biological growth patterns\n",
    "\n",
    "### 4. Reduced Major Axis (Geometric Mean) Regression\n",
    "\n",
    "This form of regression is used when the x data has uncertainty. It reduced the distance between the regression line and the data in the orthoganal direction from the line. \n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    " {\\beta_1} = sign(r) \\cdot \\sqrt{\\sigma_y^2 / \\sigma_x^2}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "To compute the intercept:\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    " {\\beta_0} = \\bar{y} - {\\beta_1} \\cdot \\bar{x}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "\n",
    "${r}$ &emsp;&emsp;correlation between the x and y<br>\n",
    "${\\sigma_x}$&emsp;&emsp; Standard deviation of x<br>\n",
    "${\\sigma_y}$&emsp;&emsp; Standard deviation of y<br>\n",
    "  \n",
    "\n",
    "\n",
    "###  5. Regularisation\n",
    "Regularisation methods use differing penalisation functions, based around 'distance' calculations of the coefficients.\n",
    "\n",
    "#### Vector norms\n",
    "Some common distance metrics (vector norms) are list here:\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "L1 norm (Manhattan distance or Taxicab distance): \n",
    "$$\n",
    "L_1 = ||\\mathbf{v}||_1 =  \\sum_{i=1}^{n} |v|\n",
    "$$\n",
    "</div>\n",
    "\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "L2 norm (Euclidean norm): \n",
    "$$\n",
    "L_2 = ||\\mathbf{v}||_2 =  \\sqrt{\\sum_{i=1}^{n} v^2}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">  \n",
    "L infintiy norm:\n",
    "$$\n",
    "L_\\infty = ||\\mathbf{v}||_\\infty =  \\max_i|v|\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "## Lasso (*L1*) Regression\n",
    "Cost Function = RSS + λ × (sum of absolute values of coefficients)\n",
    "$$\n",
    "Cost Function = \\sum_{i=1}^{n}(y - \\hat{y_i} )^2 + \\alpha L_1\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ (alpha) is the regularization parameter.\n",
    "\n",
    "A higher value of $\\alpha$ increases the penalty on large coefficients.  \n",
    "  \n",
    "Lasso regression needs to solved using a gradient descent type algorithm.\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "lasso_reg.predict([[1.5]])\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(penalty=\"l1\")\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[1.5]])\n",
    "```\n",
    "\n",
    "## Ridge (*L2*) Regression\n",
    "Cost Function = RSS + $\\alpha$ ×  (sum of squared values of coefficients = ($L_2^2$)  )   \n",
    "(N.B. the removal of the square root)  \n",
    "\n",
    "$$\n",
    "Cost Function = \\sum_{i=1}^{n}(y - \\hat{y_i} )^2 + \\alpha L_2^2\n",
    "$$\n",
    "Where:\n",
    "- $\\alpha$ (alpha) is the regularization parameter\n",
    "\n",
    "A higher value of $\\alpha$ increases the penalty on large coefficients.\n",
    "\n",
    "Ridge regression has a closed form.\n",
    "To solve for ${\\beta}$:\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "\\boldsymbol{\\beta} = (\\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{A})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "</div>\n",
    "  \n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "\\mathbf{A} = \\begin{pmatrix}\n",
    "0 & 0 & 0 & \\cdots & 0 \\\\\n",
    "0 & 1 & 0 & \\cdots & 0 \\\\\n",
    "0 & 0 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\cdots & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "</div>\n",
    "In $\\mathbf{A}$ the top left entry is 0 so we do not penalise the offset (intercept) value.\n",
    "  \n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])\n",
    "```\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(penalty=\"l2\")\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[1.5]])\n",
    "```\n",
    "## Elasticnet (*L1 & L2*) Regression\n",
    "$$\n",
    "Cost Function = \\sum_{i=1}^{n}(y - \\hat{y_i} )^2 + {r}\\alpha L_1 + {(1 - r)}\\alpha L_2^2\n",
    "$$\n",
    "\n",
    "In long form: \n",
    "$$\n",
    "Cost Function = \\sum_{i=1}^{n}(y - \\hat{y_i} )^2 + {r}\\alpha \\sum_{i=1}^{n} |\\beta| + {(1 - r)}\\alpha \\sum_{i=1}^{n} \\beta^2\n",
    "$$\n",
    "   \n",
    "Where:\n",
    "- ${r}$ is the mix ratio of ${L1}$ to ${L2}$\n",
    "  \n",
    "```python\n",
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net.fit(X, y)\n",
    "elastic_net.predict([[1.5]])\n",
    " ```\n",
    "  \n",
    " ### 6. Preprocessing methods\n",
    "Preprocess of the columns (predictor variables) or rows (samples) of the X data can help the modelling process.\n",
    "\n",
    "#### Mean centering\n",
    "Subtraction of the mean of a column from each value in the column is called *mean-centering*\n",
    "\n",
    "$$\\mathbf{X}_{\\text{centered}} = \\mathbf{X} -   \\mathbf{1}_m \\otimes  \\mathbf{\\bar{x}}$$ \n",
    "  \n",
    "-  $\\mathbf{\\bar{x}}$  is a vector of the means of each column of $\\mathbf{X}$  \n",
    "-  $\\mathbf{1}_m = \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "(N.B. if modelling the intercept, then the column of 1's should remain unchanged) \n",
    "\n",
    "#### Variance Scaling (or standardisation)\n",
    "Division of each (mean centred) column by the variance of the column is called *variance scaling*\n",
    "\n",
    " $$\\mathbf{X}_{\\text{scaled}} = \\frac{\\mathbf{X} - \\mathbf{1}_m {\\mu}}{\\mathbf{1}_m {\\sigma}}$$\n",
    "\n",
    "\n",
    "#### Vector normalisation\n",
    "   \n",
    " $$||\\mathbf{v}|| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}$$\n",
    "\n",
    "Division of a row by its length can help scale each sample to the same range.\n",
    "\n",
    "### 7. Statistical test metrics\n",
    "  \n",
    "There are many statistical tests for determining a significant result. They depend on specification of a *Null Hypothesis*, which is the statement of no change. e.g. a mean and standard deviation that descibes the data of the population remains the same after a certain treatment. Rejection of the null-hypothesis indicates that the data found after a treatment is statisctically different from the initial population.  \n",
    "\n",
    "### Linear regression, Pearson's r coefficient and the ${R}^2$ coefficient of determination:\n",
    "The Pearson's ${r}$ correlation coefficientit measures the strength and direction of a correlation and is normalised between:\n",
    "- 1 for perfect +ve correlation\n",
    "- 0 for no correlation\n",
    "- -1 for perfect inverse or negative correlation\n",
    "It can be thought of as a normalised correlation.\n",
    "\n",
    "$$  \n",
    "{r} = \\frac{cov(x,y)}{stddev(x) \\cdot stdev(y)}\n",
    "$$\n",
    "\n",
    "${stddev(x)} = ({\\frac {\\sum ({x}-\\bar{x})^2}{m-1}})^{\\frac{1}{2}}$\n",
    "\n",
    "${cov(x,y)} = \\frac {\\sum ({x}-\\bar{x})({y}-\\bar{y})}{m-1}$\n",
    "\n",
    "\n",
    "- ${x}$ and ${y}$ are the values of the two variables of interest.  \n",
    "- ${m}$ is the number of samples of the variables of interest.\n",
    "  \n",
    "\n",
    "${R^2}$ is the Pearsons correlation coefficient squared for a single independent variable, however it is the sum of the variance of each additional independent variable, so the equivalence ends with multiple predictor variables.\n",
    "\n",
    "  \n",
    "${R}^2$ is the measure of variance in the dataset explained by the independent variable.\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "{R}^2 = 1 - \\frac{\\sum_{i}^{n}(y_i - \\hat{y_i} )^2}{ \\sum_{i}^{n}(y_i - \\bar{y} )^2}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "- $n$ &emsp;&emsp; Is the number of samples (y values)  \n",
    "- $\\hat{y_i}$&emsp;&emsp; Is the predicted value of $y$ of sample $i$  \n",
    "- $\\bar{y}$  &emsp;&emsp; Is the mean value of $y$  \n",
    "\n",
    "${R^2}$ is the amount of variance (as a proportion of the total variance) that the independent variable explains in the variation of the depenent variable.\n",
    "Please note, &emsp; \n",
    "\n",
    "#### p-values\n",
    " A p-value is *the probability of rejecting a null-hypothesis that is true*. You want this probability to be small, to say that the data shows a difference due to a treatment.\n",
    "  \n",
    "  If a p-value is small, then it is less likely that the Null-hypothesis is true.\n",
    "\n",
    "##### One-sample t-Test:\n",
    "$$\n",
    "t = \\frac{\\bar{x} - \\mu} {s / \\sqrt{n}}\n",
    "$$\n",
    "\n",
    "$\\bar{x}$ = sample mean  \n",
    "$\\mu$ = population mean  \n",
    "$s$ = sample standard deviation  \n",
    "$n$ = sample size  \n",
    "  \n",
    "```python\n",
    "from scipy import stats\n",
    "# Hypothesized population mean\n",
    "mu_0 = 3.0\n",
    "# Perform the 1-sample t-test\n",
    "t_statistic, p_value = stats.ttest_1samp(data, mu_0)\n",
    "\n",
    "```\n",
    "##### Two sample t_Test:  \n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar{x_1} - \\bar{x_2} } {\\sqrt{ \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n",
    "$$\n",
    "\n",
    "  \n",
    "```python\n",
    "from scipy import stats\n",
    "# Hypothesized population mean\n",
    "mu_0 = 3.0\n",
    "# Perform the 2-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(data1, data2)\n",
    "\n",
    "```\n",
    "  [ref: www.cuemath.com](https://www.cuemath.com/t-test-formula/)\n",
    "  \n",
    "  \n",
    "#### Confidence intervals on estimates\n",
    "  \n",
    "For a choosen significance level, with degrees of freedom = ${n}-2$\n",
    "\n",
    "$$\n",
    "CSS_x = \\sum_{i=1}^{n} {x^2} - \\frac {( \\sum_{i=1}^{n} {x} )^2  } {n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta\\beta_1 =  \\pm t_{(\\alpha/2, {n}-2)} \\sqrt{\\frac{s^2}{CSS_x} }\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta\\beta_0 =  \\pm t_{(\\alpha/2, {n}-2)} \\sqrt{\\frac{s^2 \\cdot \\sum_{i=1}^{n} {x^2} }{n \\cdot CSS_x} }\n",
    "$$\n",
    "\n",
    "- $CSS_x$ is the corrected sum of squares\n",
    "- $\\Delta\\beta_1$ is the error range of the slope \n",
    "- $\\Delta\\beta_0$ is the error range of the intercept\n",
    "- $s^2$ is the variance\n",
    "  \n",
    "## Conclusion\n",
    "\n",
    "Linear regression is a powerful tool for modelling relationships between variables. Its simplicity, interpretability, and efficiency make it an essential algorithm in the machine learning toolkit. While it has limitations in modelling complex, non-linear relationships, variations like polynomial regression can help address some of these limitations.\n",
    "\n",
    "The choice between simple, multiple, or polynomial regression depends on the specific problem, the relationship between variables, and the complexity of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b02271e-6ead-40b3-a408-5185f0059cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
