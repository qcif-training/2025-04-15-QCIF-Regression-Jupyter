{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adef47f-81e5-4986-a973-27c9a11f2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "user = os.getenv('USER')\n",
    "os.chdir(f'/scratch/cd82/{user}/notebooks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f006ce3-0090-46f9-bac3-cc8a7575522b",
   "metadata": {},
   "source": [
    "## Linear Regression - Multiple Linear Regression (MLR)\n",
    "In this section:\n",
    "- We look at models with more than a single independant/predictor variable.\n",
    "- We will use a train-test split of the data so as to test our model.\n",
    "- We also look at how to select the best predictor variables using t-test scores and their p-values\n",
    "- We will look at automated methods of reducing the number of predictor variables using *Regularisation*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9543c0d0-9299-4d87-8da3-c54eaabbab30",
   "metadata": {},
   "source": [
    "#### The multiple linear regression equation:\n",
    "\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "{y} = {\\beta}_{0} + {x}_{1} {\\beta}_{1} + {x}_{2} {\\beta}_{2}  + ... + {x}_{n} {\\beta}_{n}  + {\\epsilon}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "Where:\n",
    "- ${y}$ is the dependent variable that we're trying to predict\n",
    "- ${x}_{1}$, ${x}_{2}$, ..., ${x}_{n}$ are the independent variables\n",
    "- ${\\beta}_{0}$ is the y-intercept (bias)\n",
    "- ${\\beta}_{1}$, ${\\beta}_{2}$, ..., ${\\beta}_{n}$ are the coefficients (weights) for each feature\n",
    "- ${\\epsilon}$ is the error term\n",
    "<br>\n",
    "**In matrix form**:\n",
    "\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "</div>\n",
    "    \n",
    "$\\mathbf{y}$ &emsp;&emsp;A (column) vector of responses<br>\n",
    "$\\mathbf{X}$&emsp;&emsp; A matrix of independent variables  \n",
    "                          (row for each sample, column for each independant variable)<br>\n",
    "$\\boldsymbol{\\beta}$&emsp;&emsp; A vector of coefficients.<br>\n",
    " \n",
    "#### Equation to solve for ${\\beta}$:\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "\\boldsymbol{\\beta} = (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4d17a7-98df-474d-b19a-cfcf51143478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b99874-52a0-42ed-9c48-be7938ea9b8b",
   "metadata": {},
   "source": [
    "### Generate data\n",
    "Again, our example is going to use synthetic data i.e. data derived from a known distribution.  \n",
    "This time we will use the Skikit-Learn function ```make_regression```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3587519e-2aad-4b96-8441-57f40b74d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(\n",
    "    n_samples=200, \n",
    "    n_features=10, \n",
    "    n_informative=5, \n",
    "    noise=5,\n",
    "    bias=100.0,\n",
    "    random_state=42)\n",
    "\n",
    "# Add a constant to the model (intercept)\n",
    "X_int = sm.add_constant(X)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Create a box and whisker plot for each feature\n",
    "X_df = pd.DataFrame(X_int)\n",
    "\n",
    "# Create a box and whisker plot for each feature\n",
    "X_df.boxplot()\n",
    "plt.title('Synthetic MLR data')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Values')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4c4477-ad8d-40de-88fa-4b1ae04f09e2",
   "metadata": {},
   "source": [
    "##### Plot the Y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2537a914-8752-4159-97c6-fae291e51d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "# Create a box and whisker plot for each feature\n",
    "# X_df = pd.DataFrame(X_int)\n",
    "y_df = pd.DataFrame(y, columns=['y'])\n",
    "\n",
    "# Create a box and whisker plot for each feature\n",
    "y_df.boxplot()\n",
    "plt.title('Y data')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Values')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d694b4-2c2d-429a-be99-7eee3e758428",
   "metadata": {},
   "source": [
    "#### Splitting our input data\n",
    "We will split our dataset into a *training* set and a *testing* set.\n",
    "- This helps prevent our models from being 'over-fit' by the training data.\n",
    "- The split out data means we can validate our model with data that is external to the data the model was trained on.\n",
    "- It helps to produce models that are transferable to new data.\n",
    "\n",
    "Over-fitting datasets is not such a problem in linear regression methods, however gets more important in more complex machine learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b41ce-6777-4e61-b053-15f18709276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "# from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_int, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42)\n",
    "\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca42bc-dd0a-46b5-8b5e-051bc1222b32",
   "metadata": {},
   "source": [
    "#### MLR regression using ```statsmodels```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeefb56e-9e8a-494e-894b-ff80f7987a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = sm.OLS(y_train,X_train)\n",
    "results_mlr = model.fit()\n",
    "\n",
    "# Get the R-squared value\n",
    "r_squared = results_mlr.rsquared\n",
    "print('R sqrd (extracted):', r_squared)\n",
    "\n",
    "# Print the summary of the model\n",
    "print(results_mlr.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d032b99-7a7e-4ae6-a45b-2c247c4efab2",
   "metadata": {},
   "source": [
    "N.B. Hight absolute  t-test scores and small p-values indicate the predictor variable is significant - i.e.  the variable is contributing to the predictive power of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db6feb-e031-4089-8613-2f545f7569f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_mlr = results_mlr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf0ecb-3f3c-4fc3-96a0-a575c677d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "y_test_ave = np.average(y_train) \n",
    "mse = mean_squared_error(y_test, y_pred_mlr)\n",
    "r2 = r2_score(y_test, y_pred_mlr)\n",
    "\n",
    "print(f\"Average Y value: {y_test_ave}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Error: {np.sqrt(mse)}\")\n",
    "print(f\"RÂ² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e34403-6749-404d-a773-09ff012dfcf5",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213cd8ab-8695-4b35-9686-aa46c933a73c",
   "metadata": {},
   "source": [
    "Slightly modified the above code to add `residuals` and model `coefficients`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e8756f-6d7c-4342-92d8-807a76add594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added residuals\n",
    "residuals = y_test - y_pred_mlr\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred_mlr)\n",
    "r2 = r2_score(y_test, y_pred_mlr)\n",
    "# coefficients = model.coef_\n",
    "coefficients = results_mlr.params\n",
    "# Create a DataFrame with coefficients and p-values\n",
    "summary_table = pd.DataFrame({\n",
    "    'Coefficient': results_mlr.params,\n",
    "    'P-Value': results_mlr.pvalues\n",
    "})\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee067b-32a9-499a-b9d4-5f4a6783758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "# Coefficients plot\n",
    "# axs[0].bar(X.columns, coefficients)\n",
    "bars = axs[0].bar(summary_table.index, summary_table['Coefficient'], color='skyblue')\n",
    "axs[0].set_title('Feature Coefficients')\n",
    "\n",
    "# Add p-values on each bar\n",
    "for bar, p_value in zip(bars, summary_table['P-Value']):\n",
    "    height = bar.get_height()\n",
    "    axs[0].text(bar.get_x() + bar.get_width() / 2, height,\n",
    "            f'{p_value:.2e}', ha='center', va='bottom')\n",
    "\n",
    "axs[0].set_xlabel('Variables')\n",
    "axs[0].set_ylabel('Coefficients')\n",
    "axs[0].set_title('Coefficients with P-Values')\n",
    "\n",
    "axs[0].set_xlabel('Features')\n",
    "axs[0].set_ylabel('Coefficients')\n",
    "\n",
    "# Residuals vs Predictions plot\n",
    "axs[1].scatter(y_pred_mlr, residuals)\n",
    "\n",
    "axs[1].set_title('Residuals vs Predictions')\n",
    "axs[1].set_xlabel('Predictions')\n",
    "axs[1].set_ylabel('Residuals')\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d8b1bd-61e4-448d-ae57-5a3d48323c93",
   "metadata": {},
   "source": [
    "The first plot is a bar chart of feature coefficients. This gives a good view of the effect each feature has on the prediction. A feature with a higher coefficient has a larger effect than a feature with a lower coefficient.\n",
    "\n",
    "The second plot is a scatter plot of residuals versus predictions. Ideally, the residuals should be randomly spread around the centerline. If there are any patterns in the residuals, it signals that the model could be improved by including non-linear terms or interaction terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a68de94-f6ed-4ce1-8684-f682ba217bf8",
   "metadata": {},
   "source": [
    "###  Regularisation\n",
    "Regularisation methods use differing penalisation functions, based around 'distance' calculations of the coefficients.\n",
    "\n",
    "#### Vector norms\n",
    "Some common distance metrics (vector norms) are list here:\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "L1 norm (Manhattan distance or Taxicab distance): \n",
    "$$\n",
    "L_1 = ||\\mathbf{v}||_1 =  \\sum_{i=1}^{n} |v|\n",
    "$$\n",
    "</div>\n",
    "\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "L2 norm (Euclidean norm): \n",
    "$$\n",
    "L_2 = ||\\mathbf{v}||_2 =  \\sqrt{\\sum_{i=1}^{n} v^2}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">  \n",
    "L infintiy norm:\n",
    "$$\n",
    "L_\\infty = ||\\mathbf{v}||_\\infty =  \\max_i|v|\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "## Lasso (*L1*) Regression\n",
    "Cost Function = RSS + Î» Ã (sum of absolute values of coefficients)\n",
    "$$\n",
    "Cost Function = \\sum_{i=1}^{n}(y - \\hat{y_i} )^2 + \\alpha L_1\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ (alpha) is the regularization parameter.\n",
    "\n",
    "A higher value of $\\alpha$ increases the penalty on large coefficients.  \n",
    "  \n",
    "Lasso regression needs to solved using a gradient descent type algorithm.\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "lasso_reg.predict([[1.5]])\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(penalty=\"l1\")\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[1.5]])\n",
    "```\n",
    "\n",
    "## Ridge (*L2*) Regression\n",
    "Cost Function = RSS + $\\alpha$ Ã  (sum of squared values of coefficients = ($L_2^2$)  )   \n",
    "(N.B. the removal of the square root)  \n",
    "\n",
    "$$\n",
    "Cost Function = \\sum_{i=1}^{n}(y - \\hat{y_i} )^2 + \\alpha L_2^2\n",
    "$$\n",
    "Where:\n",
    "- $\\alpha$ (alpha) is the regularization parameter\n",
    "\n",
    "A higher value of $\\alpha$ increases the penalty on large coefficients.\n",
    "\n",
    "Ridge regression has a closed form.\n",
    "To solve for ${\\beta}$:\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "\\boldsymbol{\\beta} = (\\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{A})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "</div>\n",
    "  \n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "\\mathbf{A} = \\begin{pmatrix}\n",
    "0 & 0 & 0 & \\cdots & 0 \\\\\n",
    "0 & 1 & 0 & \\cdots & 0 \\\\\n",
    "0 & 0 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\cdots & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "</div>\n",
    "In $\\mathbf{A}$ the top left entry is 0 so we do not penalise the offset (intercept) value.\n",
    "  \n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])\n",
    "```\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(penalty=\"l2\")\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[1.5]])\n",
    "\n",
    "\n",
    "```\n",
    "## Elasticnet (*L1 & L2*) Regression\n",
    "$$\n",
    "Cost Function = \\sum_{i=1}^{n}(y - \\hat{y_i} )^2 + {r}\\alpha L_1 + {(1 - r)}\\alpha L_2^2\n",
    "$$\n",
    "\n",
    "In long form: \n",
    "$$\n",
    "Cost Function = \\sum_{i=1}^{n}(y - \\hat{y_i} )^2 + {r}\\alpha \\sum_{i=1}^{n} |\\beta| + {(1 - r)}\\alpha \\sum_{i=1}^{n} \\beta^2\n",
    "$$\n",
    "   \n",
    "Where:\n",
    "- ${r}$ is the mix ratio of ${L1}$ to ${L2}$\n",
    "  \n",
    "```python\n",
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net.fit(X, y)\n",
    "elastic_net.predict([[1.5]])\n",
    "\n",
    "# or using statsmodels\n",
    "import statsmodels.api as sm\n",
    "# L1_wt == 1.0 then it is Lasso\n",
    "# L1_wt == 0.0 then it is Ridge regression\n",
    "lasso_model = sm.OLS(y, X).fit_regularized(method='elastic_net', \n",
    "                                         alpha=0.1, L1_wt=1.0)\n",
    "\n",
    "ridge_model = sm.OLS(y, X).fit_regularized(method='elastic_net', \n",
    "                                         alpha=0.1, L1_wt=0.0)\n",
    " ```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3308d11c-1c3a-48c3-a0dc-7b0fe961ed6c",
   "metadata": {},
   "source": [
    "#### Selection of coefficents in the original dataset using Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ffcd1-d90e-41db-b29e-e2f28797fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = sm.OLS(y_train, X_train).fit_regularized(method='elastic_net', \n",
    "                                         alpha=1.0, L1_wt=1.0)\n",
    "\n",
    "non_zero_coefficients = np.sum(lasso_model.params != 0)\n",
    "print(f\"Number of non-zero coefficients: {non_zero_coefficients}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c7519-9660-4a00-a896-e84272467b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_test_pred = lasso_model.predict(X_test)\n",
    "# print(\"Predictions:\", y_test_pred)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"RÂ² Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e985106-51c3-43da-8ab7-f81808f02b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract non-zero coefficients\n",
    "non_zero_indices = np.where(lasso_model.params != 0)[0]\n",
    "selected_predictors = X_train[:, non_zero_indices]  # subset our full dataset with our selected \n",
    "\n",
    "# Optionally refit the model with selected predictors\n",
    "refit_model = sm.OLS(y_train, selected_predictors).fit()\n",
    "\n",
    "print(\"Selected predictors:\", non_zero_indices)\n",
    "print(\"Refitted model summary:\", refit_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d034b52-0fbf-4f7c-84b1-b90159e198f3",
   "metadata": {},
   "source": [
    "#### Finding the best values in the regularisation\n",
    "As we now have various hyperparameters to choose, the modelling task gets a little more laborious. Tha is where GridSearchCV comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc4494e-92ed-440a-ba43-dc6f2942f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Set up the parameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.25, 0.5, 0.75, 1.0, 5.0, 10.0, 20.0],\n",
    "}\n",
    "\n",
    "# Initialize the Lasso model\n",
    "lasso = Lasso()\n",
    "\n",
    "# Use GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(estimator=lasso, param_grid=param_grid, \n",
    "                           cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score (r^2): \", grid_search.best_score_)\n",
    "\n",
    "# Fit the model with the best parameters\n",
    "best_lasso = grid_search.best_estimator_\n",
    "best_lasso.fit(X_train, y_train)\n",
    "\n",
    "coefficients = best_lasso.coef_\n",
    "non_zero_indices = np.where(lasso_model.params != 0)[0]\n",
    "print(\"non_zero_indices coefficients:\", non_zero_indices)\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_gridcv = best_lasso.predict(X_test)\n",
    "\n",
    "# print(\"Predictions on test data: \", y_pred_gridcv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be4e51-af75-4780-85ed-9f8b32aa5d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv_cmdstan)",
   "language": "python",
   "name": "myenv_cmdstan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
