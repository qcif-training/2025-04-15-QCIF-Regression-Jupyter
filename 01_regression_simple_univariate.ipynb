{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74153b-aad0-460d-b091-1050a681f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "user = os.getenv('USER')\n",
    "os.chdir(f'/scratch/cd82/{user}/notebooks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09e1a96-4780-4fe8-9717-2b16a950a592",
   "metadata": {},
   "source": [
    "# Linear Regression - \n",
    "This workbook is an introduction to the equations and the mathmatics that describe and compute the parameters to fit linear models.\n",
    "The maths used is the 'long-hand` way to get a result uding ```NumPy``` matrix operations. Packages such as ```Scikit-Learn``` and ```statsmodels``` simplify the process and provide a suite of the statistical computations to determine the validity of a proposed model. These notes are just to give you an idea of what is happening 'under the hood' of these packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7893220-e825-4087-96dc-4e849bf4a921",
   "metadata": {},
   "source": [
    "### Simple linear regression (no intercept):\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "{y} =  {x}_{1} {\\beta}_{1} + {\\epsilon}\n",
    "$$\n",
    "</div>\n",
    "${y}$&emsp;&emsp;The dependent variable (response variable)<br>\n",
    "${x}_{1}$&emsp;&emsp;The independent variable<br>\n",
    "${\\beta}_{1}$&emsp;&emsp;The regression coefficient (also known as the slope). <br>\n",
    "&emsp;&emsp;&emsp;&emsp;(In artificial neural networks, ${\\beta}$ are called the weights)<br>\n",
    "${\\epsilon}$&emsp;&emsp;&emsp;The residual (error)<br>\n",
    "<div><br></div>\n",
    "  \n",
    "**In matrix form**:\n",
    "\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "</div>\n",
    "    \n",
    "$\\mathbf{y}$ &emsp;&emsp;A (column) vector of responses<br>\n",
    "$\\mathbf{X}$&emsp;&emsp; A matrix of independent variables  \n",
    "                          (row for each sample, column for each independant variable)<br>\n",
    "$\\boldsymbol{\\beta}$&emsp;&emsp; A vector of coefficients.<br>\n",
    " \n",
    "#### Equation to solve for ${\\beta}$:\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "\\boldsymbol{\\beta} = (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3543ef7-6881-4707-b868-c5e418e6fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2dcde5-a819-495f-a464-f727f9facd92",
   "metadata": {},
   "source": [
    "#### Generate Data\n",
    "Our first examples are going to use synthetic data i.e. data derived from a known distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7b1a70-ed6f-45b5-8959-846100f28bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "\n",
    "N = 40  # the number of samples to be created\n",
    "\n",
    "# The seed for the random number generator.\n",
    "seed_seq = np.random.SeedSequence(42) \n",
    "# Create a random number generator instance\n",
    "rng = np.random.default_rng(seed_seq)\n",
    "\n",
    "rndg = rng.normal(loc=0.0, scale=1, size=N)\n",
    "rndg = rndg.reshape((N, 1))\n",
    "print('rndg shape: ', rndg.shape)\n",
    "\n",
    "# Create X data\n",
    "start_x = 2.0\n",
    "range_x = 2.0\n",
    "\n",
    "X = np.linspace(start_x, start_x+range_x, num=N )\n",
    "X = X.reshape((N, 1))\n",
    "print('X shape: ', X.shape)\n",
    "\n",
    "pc_rand = 0.75  # +- how much randomness to be added to y data\n",
    "\n",
    "# Create y data\n",
    "offset_y = 6.0\n",
    "slope_y = 4.0\n",
    "\n",
    "add_offset = (start_x * slope_y) + offset_y\n",
    "# add_offset =  offset_y\n",
    "y = (( add_offset )+ slope_y * (X - start_x)) + (pc_rand * rndg)\n",
    "print('y shape: ',y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b8773-e7fa-445d-bff1-889bd5c1f5d4",
   "metadata": {},
   "source": [
    "#### Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3763e4-e61e-4a8d-aadc-e6e26c803ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max = np.max(X) \n",
    "x_min = np.min(X) \n",
    "y_max = np.max(y) \n",
    "y_min = np.min(y) \n",
    "pe = 0.5  # extend the plot axis by this much in each direction\n",
    "\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# First subplot: Box and whisker plot with scatter\n",
    "X_df = pd.DataFrame(X)\n",
    "y_df = pd.DataFrame(y, columns=['y'])\n",
    "Xy_df = pd.concat([X_df, y_df], axis=1)\n",
    "Xy_df.boxplot(ax=ax1)\n",
    "\n",
    "# Plot the actual points\n",
    "ax1.scatter(np.ones_like(X), X, alpha=0.6)\n",
    "ax1.scatter(np.ones_like(X)+1, y, alpha=0.6)\n",
    "\n",
    "ax1.set_title('Synthetic univariate data')\n",
    "ax1.set_xticks([1, 2])\n",
    "ax1.set_xticklabels(['X', 'y'])\n",
    "ax1.set_ylabel('Values')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Second subplot: Scatter plot of y vs X\n",
    "ax2.scatter(X, y, color='blue', label='y data')\n",
    "ax2.axis([0.0, x_max+pe, 0.0, y_max+pe])\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('y vs X')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "fig.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44563fc0-b11a-4099-82d7-37c4ebde52cc",
   "metadata": {},
   "source": [
    "#### Create our model\n",
    "We will be fitting the data to a single $\\beta$ coefficient.  \n",
    "Just as a reminder:\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "\\boldsymbol{\\beta} = (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da0a769-af16-4518-a569-3e416ac3390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the beta \n",
    "# We are using the whole data set here\n",
    "dot_product = np.dot(np.transpose(X), X)\n",
    "betas_noint = np.linalg.inv(dot_product) * np.dot(np.transpose(X), y)\n",
    "print('betas[0]: ', betas_noint[0])\n",
    "\n",
    "# Predict our dependent value with our model\n",
    "y_pred = betas_noint[0] * X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa529a-5330-4f98-94fd-39f30b48a4d1",
   "metadata": {},
   "source": [
    "##### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c7d1df-821c-43d8-9bbf-c407e46933b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X, y, color='blue', s=12, label='y data')\n",
    "# plt.scatter(X, y_pred, color='red',marker='x',  label='predicted y')\n",
    "\n",
    "X_extended = np.linspace(0, max(X), 100)\n",
    "y_extended = betas_noint[0] * X_extended\n",
    "\n",
    "plt.plot(X_extended, y_extended, color='red', linewidth=1.0, label='Regression line')\n",
    "plt.axis([0.0, x_max+pe, 0.0, y_max+pe])\n",
    "\n",
    "# Define the points for the rise and run\n",
    "rx = [0.0, 1.0 ]  # x-coordinates\n",
    "ry = [0.0, 0.0]  # y-coordinates\n",
    "\n",
    "rux = [ 1.0, 1.0]  # x-coordinates\n",
    "ruy = [0.0, betas_noint[0][0]]  # y-coordinates\n",
    "\n",
    "# Plot the line\n",
    "# plt.plot(rx, ry, marker='o', label='run (1.0)')\n",
    "# plt.plot(rux, ruy, marker='o', label='rise ('+str(round(betas_noint[0][0],3))+')')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8da93e2-6d34-4998-8e66-e4b8d4277db8",
   "metadata": {},
   "source": [
    "## Linear regression, ${R}^2$ coefficient of determination:\n",
    "${R}^2$ is the measure of variance in the dataset explained by the independent variable.\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "{R}^2 = 1 - \\frac{\\sum_{i}^{m}(y_i - \\hat{y_i} )^2}{ \\sum_{i}^{m}(y_i - \\bar{y} )^2}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "- $m$ &emsp;&emsp;Is the number of samples (y values)  \n",
    "- $\\hat{y_i}$&emsp;&emsp; Is the predicted value of $y$ of sample $i$  \n",
    "- $\\bar{y}$  &emsp;&emsp; Is the mean value of $y$  \n",
    "\n",
    "${R^2}$ is the amount of variance (as a proportion of the total variance) that the independent variable explains in the variation of the depenent variable.\n",
    "Please note, &emsp; ${R^2}$ is the Pearsons correlation coefficient squared for a single independent variable, however it is the sum of the variance of each additional independent variable, so the equivalence ends with multiple predictor variables.\n",
    "\n",
    "Where can we find a function in Python:\n",
    "```python\n",
    "from sklearn.metrics import r2_score\n",
    "rsc = r2_score (y, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737cb8ba-48ad-40a9-aeb4-c482b4f85243",
   "metadata": {},
   "source": [
    "### Plots to show values of interest in the ${R^2}$computation\n",
    "The following plots show the differences between the data and the predictions and the data and the estimate of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4671a2fe-bc88-4347-8d9d-6030b19a9c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max = np.max(X) \n",
    "x_min = np.min(X) \n",
    "y_max = np.max(y) \n",
    "y_min = np.min(y) \n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.scatter(X, y, color='blue', label='y data',s=12)\n",
    "# plt.scatter(X, y_pred, color='red',  label='model predicted y')\n",
    "plt.plot(X, y_pred, color='red', linewidth=1.0, label='Regression line')\n",
    "\n",
    "# Plot the error lines\n",
    "for i in range(len(y)):\n",
    "    plt.plot([X[i], X[i]], [y_pred[i], y[i]], color='green')\n",
    "    \n",
    "plt.axis([x_min-pe, x_max+pe, y_min-pe, y_max+pe])\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Errors compared to predictors')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the results showing the difference between the data and the estimate of beta \n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "average_y = sum(y) / len(y)\n",
    "# Plot the average line\n",
    "plt.axhline(y=average_y, color='r', linestyle='-', label=f'Average: {average_y}')\n",
    "plt.scatter(X, y, color='blue', label='y data',s=12)\n",
    "plt.axis([x_min-pe, x_max+pe, y_min-pe, y_max+pe])\n",
    "\n",
    "for i in range(len(y)):\n",
    "    plt.plot([X[i], X[i]], [average_y, y[i]], color='green')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Errors compared to mean')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a72ff6-3f15-40c1-990a-0febe7bbc676",
   "metadata": {},
   "source": [
    "# Plot of the errors (the residuals)\n",
    "A model that has randomly distributed residuals (the $\\eta$ values) typically means that the model has accounted for the variation in the data.  \n",
    "Plotting of the errors (the differences between the prediction and the actual data) can visually indicate if there is further *structure* to the data that is not being accounted for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ab105-267a-4509-99c2-bf5e019536f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_y = sum(y) / len(y)\n",
    "\n",
    "residual_y = y - y_pred\n",
    "residual_y_ave = y - average_y\n",
    "\n",
    "x_max = np.max(X) \n",
    "x_min = np.min(X) \n",
    "y_max = np.max(residual_y) \n",
    "y_min = np.min(residual_y) \n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "\n",
    "plt.scatter(X, residual_y, color='blue', label='residual y data')\n",
    "\n",
    "plt.axis([x_min-pe, x_max+pe, y_min-pe, y_max+pe])\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('residual_y')\n",
    "plt.title('Errors compared to predictors')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the results showing the difference between the data and the estimate of beta \n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22846749-3f3c-43aa-ba16-cd41f5692292",
   "metadata": {},
   "source": [
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "{R}^2 = 1 - \\frac{\\sum_{i}^{m}(y_i - \\hat{y_i} )^2}{ \\sum_{i}^{m}(y_i - \\bar{y} )^2}\n",
    "$$\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4e959-274c-401c-8325-3f90f1cade1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Coefficient of determination\n",
    "from sklearn.metrics import r2_score\n",
    "# This is the Pearsons R function\n",
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "\n",
    "ss_residual_y = np.dot(np.transpose(residual_y), residual_y)\n",
    "ss_residual_y_ave = np.dot(np.transpose(residual_y_ave), residual_y_ave)\n",
    "\n",
    "R_sqrd = 1.0 - (ss_residual_y/ ss_residual_y_ave)\n",
    "print('R squared (computed above): ',R_sqrd)\n",
    "\n",
    "# We can compare our computed value with what Scikit-Learn computes\n",
    "rsc = r2_score (y.ravel(), y_pred)\n",
    "r_pearson = r_regression(y_pred, y.ravel())  # Pearsons correlation (standardized variance)\n",
    "print('R squared (SKL): ', rsc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d64963a-7450-4127-bbcc-9fd48e7b7e53",
   "metadata": {},
   "source": [
    "\n",
    "  \n",
    "##### Why do I have a negative ${R^2}$\n",
    "The ratio seen in the ${R^2}$ calculation can result in spurious values if the *sums of differences from the mean* are smaller than the *sums of differences from predicted* data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f162d05-afb3-4857-be2c-188b0794fcd0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def36d04-3066-452a-9901-4627154331f2",
   "metadata": {},
   "source": [
    "### Linear regression, with addition of the slope coefficient\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "{y} = {\\beta}_{0} + {x}_{1} {\\beta}_{1} + {\\epsilon}\n",
    "$$\n",
    "</div>\n",
    "$ {\\beta}_{0}$ &emsp;&emsp; Added as the intercept (also known as the bias or offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb1526-b197-4ae4-837b-c8a2d4c3eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To model the intercept, we need to add a column of 1's to the X data\n",
    "X_intercept = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "print('Shape(X)      : ',np.shape(X_intercept))\n",
    "\n",
    "# Perform the matrix operation X^T x X\n",
    "dot_product = np.dot(np.transpose(X_intercept), X_intercept)\n",
    "print('Shape(X^T x X): ', np.shape(dot_product))\n",
    "\n",
    "# Compute the coefficients\n",
    "betas = np.dot(np.linalg.inv(dot_product), np.dot(np.transpose(X_intercept), y))\n",
    "print('Shape(betas)  : ',np.shape(betas))\n",
    "print('betas[0] (intercept)  : ', betas[0])\n",
    "print('betas[1]              : ', betas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea9b84-6bba-483a-b7e9-c070712607d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max = np.max(X) \n",
    "x_min = np.min(X) \n",
    "y_max = np.max(y) \n",
    "y_min = np.min(y) \n",
    "\n",
    "\n",
    "# Plot the results - compare the two models, without and with an intercept\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "# Plot the results\n",
    "average_y = sum(y) / len(y)\n",
    "# Plot the average line\n",
    "# plt.axhline(y=average_y, color='r', linestyle='--', label=f'Average: {average_y}')\n",
    "\n",
    "y_pred = betas_noint[0] * X\n",
    "plt.scatter(X, y, color='blue', label='Actual data',s=12)\n",
    "plt.plot(X, y_pred, color='red', \n",
    "         linewidth=1, label='Regression line (no interecpt)')\n",
    "plt.axis([x_min-pe, x_max+pe, y_min-pe, y_max+pe])\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Model with the intercept\n",
    "y_pred_i = betas[0] + betas[1] * X\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X, y, color='blue', label='Actual data',s=12)\n",
    "plt.plot(X, y_pred_i, color='red', \n",
    "         linewidth=1, label='Regression line (with interecpt)')\n",
    "plt.axis([x_min-pe, x_max+pe, y_min-pe, y_max+pe])\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Simple Linear Regression, with intercept')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0e718-a630-45e4-a07c-aa7ba7bba758",
   "metadata": {},
   "source": [
    "#### Plot the residuals for new model\n",
    "We will view the ${\\epsilon}$ values from the model equation:\n",
    "<div style=\"border: 0px solid black; padding: 2px; margin: 0;\">\n",
    "$$\n",
    "{y} = {\\beta}_{0} + {x}_{1} {\\beta}_{1} + {\\epsilon}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee78e161-1ddd-490f-81d6-52a2ef8b2cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_y = sum(y) / len(y)\n",
    "residual_y_i = y - y_pred_i\n",
    "\n",
    "x_max = np.max(X) \n",
    "x_min = np.min(X) \n",
    "y_max = np.max(residual_y_i) \n",
    "y_min = np.min(residual_y_i) \n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "# Plot single coefficient model errors\n",
    "plt.scatter(X, residual_y, color='blue', label='y data',s=12)\n",
    "plt.axis([x_min-pe, x_max+pe, y_min-pe, y_max+pe])\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('residual_y_ave')\n",
    "plt.title('Errors of no-intercept model')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the results showing the difference between the data and the estimate of beta \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X, residual_y_i, color='blue', label='residual y data',s=12)\n",
    "\n",
    "plt.axis([x_min-pe, x_max+pe, y_min-pe, y_max+pe])\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('residual_y')\n",
    "plt.title('Errors of model with intercept')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b3d57-732d-4d44-a4f3-57ac1ce7bc48",
   "metadata": {},
   "source": [
    "#### The Shapiro-Wilk test for normality\n",
    "We can use Scipy library ```shapiro``` function to test if the residuals are normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47352add-21d5-4916-91bf-e800725751e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "# test of model with no intercept\n",
    "stat, p_value = shapiro(residual_y)\n",
    "\n",
    "print(\"Shapiro-Wilk Test Statistic (no intercept) :\", stat)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "stat_i, p_value_i = shapiro(residual_y_i)\n",
    "\n",
    "print(\"Shapiro-Wilk Test Statistic (with intercept):\", stat_i)\n",
    "print(\"p-value:\", p_value_i)\n",
    "\n",
    "# Values closer to 1 indicate normality, and p values e.g. > 0.05, means normality cannot be rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ebcb7-582d-4b90-9801-253f9792000b",
   "metadata": {},
   "source": [
    "#### The Pearson's ${r}$ coefficient\n",
    "\n",
    "\n",
    "For simple univariate regression, with an intercept, we can compute the ${R^2}$ value by squaring the Pearson's r correlation:\n",
    "\n",
    "$$  \n",
    "{r} = \\frac{cov(x,y)}{\\sigma(x) \\cdot \\sigma(y)}\n",
    "$$\n",
    "\n",
    "${cov(x,y)} = \\frac {\\sum ({x}-\\bar{x})({y}-\\bar{y})}{m-1}$\n",
    "\n",
    "${\\sigma(x)} = \\sqrt{{\\frac {\\sum ({x}-\\bar{x})^2}{m-1}}}$\n",
    "\n",
    "$$\n",
    "{R^2} = {r^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f739781-c11c-4f20-8e86-36dcad03b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "def persons_r_ours(x, y):\n",
    "    xy_cov = np.cov(x,y, rowvar=False, bias=True)\n",
    "    x_var = (np.std(x))  # the standard deviation\n",
    "    y_var = (np.std(y))\n",
    "    r = xy_cov[1,0] / (x_var * y_var)\n",
    "    return r\n",
    "\n",
    "r_xy1 = persons_r_ours(X, y)\n",
    "print('r_xy1 : ', r_xy1)\n",
    "\n",
    "\n",
    "# This is the Scikit-Learn Pearson's r function\n",
    "r_pearson = r_regression(X, y.ravel())\n",
    "print('r_pearson SKL: ', r_pearson)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6653ee05-6213-4728-a866-ed3f4f7fa97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Coefficient of determination\n",
    "from sklearn.metrics import r2_score\n",
    "# This is the Pearsons R function\n",
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "rsc_2 = r2_score (y.ravel(), y_pred_i)\n",
    "\n",
    "print('R squared (SKL): ', rsc_2)\n",
    "print('R squared (Pearsons sqrd): ', r_xy1* r_xy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469b8f2c-3a5a-427c-bf94-7726dc8bdaba",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9b883-6cb8-43cd-994c-a35809e21a8b",
   "metadata": {},
   "source": [
    "### Using higher level libraries\n",
    "```Scikit-Learn``` and ```statsmodels``` libraries provide a wide range of regression methods, pre-configured to compute statistical tests and provide metriscs for the model created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88d5366-1ea5-475f-ac62-5680c3e97362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Train the model\n",
    "model_skl = LinearRegression()\n",
    "model_skl.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795ad42-2f35-4fd5-b8b4-a3372ed45ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_skl = model_skl.predict(X)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(X, y, color='blue', label='Actual data',s=12)\n",
    "plt.plot(X, y_pred_skl, color='red', \n",
    "         linewidth=1, label='Regression line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618d1ad-40cd-486a-a8a2-fe2a873b98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model and compare with our original results\n",
    "mse = mean_squared_error(y, y_pred_skl)\n",
    "r2 = r2_score(y, y_pred_skl)\n",
    "\n",
    "print(f\"Intercept: {model_skl.intercept_}\")\n",
    "print(f\"Coefficient: {model_skl.coef_}\")\n",
    "\n",
    "print('betas[0] (intercept)  : ', betas[0])\n",
    "print('betas[1]              : ', betas[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca7115-e50c-43da-82e0-d22b0727e14b",
   "metadata": {},
   "source": [
    "#### Using the ```statsmodels``` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e98e4ee-da72-4379-b944-9aed00044ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X_int= sm.add_constant(X)\n",
    "ols_model = sm.OLS(y, X_int)\n",
    "\n",
    "results_sm = ols_model.fit()\n",
    "# Print the summary of the model\n",
    "print(results_sm.summary())\n",
    "\n",
    "# Make predictions - If we had some other data\n",
    "# y_pred_sm = results_sm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665bbfa-fdda-428d-b39a-7b08047cd275",
   "metadata": {},
   "source": [
    "In the results from an Ordinary Least Squares (OLS) regression using the ```statsmodels``` package, several test statistics are provided to help you understand the model's performance and the significance of the predictors. Here are some key test statistics and what they tell us: (Summary of outputs thanks to CoPilot)\n",
    "\n",
    "**Coefficients** (coef): These are the estimated values for the regression parameters. They represent the change in the dependent variable for a one-unit change in the predictor variable, holding other variables constant.\n",
    "\n",
    "**Standard Errors** (std err): These measure the variability of the coefficient estimates. Smaller standard errors indicate more precise estimates.\n",
    "\n",
    "**t-Statistics** (t): These are calculated as the coefficient divided by its standard error. They test the null hypothesis that the coefficient is equal to zero (no effect). A larger absolute value of the t-statistic indicates stronger evidence against the null hypothesis.\n",
    "\n",
    "**P-values** (P>|t|): These indicate the probability of observing the given t-statistic, assuming the null hypothesis is true. Smaller p-values (typically < 0.05) suggest that the corresponding predictor is statistically significant.\n",
    "\n",
    "**R-squared** (R²): This measures the proportion of the variance in the dependent variable that is explained by the independent variables. Values range from 0 to 1, with higher values indicating a better fit.\n",
    "\n",
    "**Adjusted R-squared**: This is a modified version of R-squared that adjusts for the number of predictors in the model. It is useful for comparing models with different numbers of predictors.\n",
    "\n",
    "**F-statistic**: This tests the overall significance of the model. It compares the model with no predictors (intercept only) to the specified model. A higher F-statistic indicates that the model provides a better fit than the intercept-only model.\n",
    "\n",
    "**Prob (F-statistic)**: This is the p-value associated with the F-statistic. It tests the null hypothesis that all regression coefficients are equal to zero. A small p-value suggests that at least one predictor is significantly related to the dependent variable.\n",
    "\n",
    "**Akaike Information Criterion** (AIC) and **Bayesian Information Criterion** (BIC): These are measures of model quality that penalize for the number of predictors. Lower values indicate a better model fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cc999d-f27e-4955-b960-ef241c6b98a2",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "In this workbook we have:\n",
    "- Created a synthetic dataset that relates y to X, with some added noise.\n",
    "- Using Numpy matrix primitives, we created two competing models:  \n",
    "  ${y} = {x}_{1} {\\beta}_{1} + {\\epsilon}$  \n",
    "  ${y} = {\\beta}_{0} + {x}_{1} {\\beta}_{1} + {\\epsilon}$\n",
    "- We compare their ${R^2}$ values which compares the predicted values to the simplest model, that being the mean of the data (${\\bar{y}}$).\n",
    "- We discussed some possible issues with ${R^2}$ and we looked at Pearson's ${r}$ as an alternative for univariate models with an intercept.\n",
    "- We plotted the data, the predicted regression model and the residuals to check our results.\n",
    "- We used the ```Scikit-Learn``` library to perform the modelling and showed the predicted $\\beta$ values were the same.\n",
    "- We used the ```statsmodels``` library and also got a lot of summary statistics to go with the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc73a9-43ef-4601-a904-f60274fa67a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv_cmdstan)",
   "language": "python",
   "name": "myenv_cmdstan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
